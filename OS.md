# 1. 操作系统的基本概念
## 1.1 操作系统的基本特征
### 1.1.1 **并发**
1. 并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。并行性是指两个或多个时间在同一时刻发生，并发性是指两个或多个事件在同一时间间隔内发生。
2. 并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。
3. 操作系统通过引入进程和线程，使得程序能够并发运行。

### 1.1.2 **共享**
1. 共享是指系统中的资源可以被内存中的多个并发进程共同使用。
2. 存在两种共享方式：互斥共享和同时共享。
3. 互斥共享的资源称为**临界资源**，例如打印机等，在同一时间只允许一个进程访问，需要用同步机制实现对临界资源的访问。
4. 资源共享是以进程的并发执行为条件的，若系统不允许并发执行也就不存在资源共享问题；若系统不能对资源共享实施有效管理，以协调好诸进程对共享资源的访问，也会影响到诸进程间并发执行的速度。

### **虚拟**
1. 虚拟技术把一个物理实体转换为多个逻辑实体。
2. 主要有两种虚拟技术：**时分复用技术**和**空分复用技术**。<br>
3. 多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占有处理器，每次只执行一小个时间片段并快速切换。<br>
4. 虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行**页面置换**算法，将该页置换到内存中。<br>

### **异步**
异步是指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。

## 1.2 操作系统的基本功能
### 1.2.1 进程管理/处理机管理功能
1. **进程控制**
2. **进程同步**
	1. 进程互斥方式
	2. 进程同步方式
3. **进程通信**
	1. 实现相互合作进程之间的信息交换
4. **死锁处理**
5. **处理机调度**

### 1.2.2 内存管理
1. **内存分配**
2. **地址映射**
3. **内存保护与共享**
4. **虚拟内存**

### 1.2.3 文件管理
文件存储空间的管理、目录管理、文件读写管理和保护等。
### 1.2.4 设备管理
完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率，主要包括缓冲管理、设备分配、设备处理、虚拟设备等。

## 1.3 系统调用
如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。<br>
Linux 的系统调用主要有：

Task|Commands
-|-
进程控制|fork();exit();wait();
进程通信|pipe();shmget();mmap();
文件操作|open();read();write();
设备操作|ioctl();read();write();
信息维护|getpid();alarm();sleep();
安全|chmod();umask();chown();

## 1.4 大内核与微内核
### 1.4.1 大内核
大内核是将操作系统功能作为一个紧密结合的整体放到内核。由于各模块共享信息，因此有很高的性能。
### 1.4.2 微内核
由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。<br>
在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。<br>
因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。<br>

## 1.5 中断
### 1.5.1 外中断
在CPU执行指令以外的时间引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等。
### 1.5.2 异常
由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。
### 1.5.3 陷入
在用户程序中使用系统调用。


# 2. 进程(Process)与线程(Thread)
## 2.1 进程与线程的概念
### 2.1.1 进程与线程
**进程**是执行中一段程序，即一旦程序被载入到内存中并准备执行，它就是一个进程。进程是表示资源分配的的基本概念，又是调度运行的基本单位，是系统中的并发执行的单位。<br>
程序并不能单独运行，只有将程序装载到内存中，系统为它分配资源才能运行，而这种执行的程序就是进程。<br>
程序和进程的区别就在于：程序是指令的集合，它是程序运行的静态描述文本；进程是程序的一次执行活动，属于动态概念。<br>
**线程**单个进程中执行中每个任务就是一个线程。线程是进程中执行运算的最小单位。<br>

### 2.1.2 一个进程都会拥有什么资源？
1. 用户的地址空间；
2. 实现进程(线程)间同步和通信的机制；
3. 已经打开的文件和已经申请到的I/O设备；
4. 一张由核心进程维护的地址映射表(实现用户程序的逻辑地址到其内存物理地址的映射)。

### 2.1.3 进程和线程的状态
#### 2.1.3.1 进程状态及状态转换
1. **就绪**状态：进程已经处于准备好运行的状态，即进程已经分配到除CPU以外的所有必要资源，只要再获得CPU，便可以立即执行。
2. **执行**状态：进程已经获得CPU，正在执行。
3. **阻塞**状态：正在执行的进程由于发生某些事件(I/O请求、申请缓冲区失败等)暂时无法继续执行，进程的执行受到阻塞。

#### 2.1.3.2 引入挂起、激活操作后的进程状态及转换
1. 为了系统和用户观察和分析进程的需要(终端用户的需要、父进程请求、负荷调节的需要、操作系统的需要)，引入挂起操作和激活操作。
2. 挂起操作用于某进程时，该进程将被挂起，意味着此时该进程处于静止状态。如果进程正在执行，它将暂停执行。如果进程原本处于就绪状态，则该进程此时暂不接受调度。
3. 引入挂起和激活操作后的进程状态可分为静止就绪、活动就绪、执行、静止阻塞和活动阻塞。

#### 2.1.3.3 线程状态及状态转换
1. **就绪**状态：线程已经具备了各种执行条件，只需要在获得CPU便可以立即执行。
2. **执行**状态：线程已经获得CPU，正在执行。
3. **阻塞**状态：正在执行的线程由于发生某些事件(I/O请求、申请缓冲区失败等)暂时无法继续执行，线程的执行受到阻塞。(比如当一个 )


#### 2.1.3.4 线程各个状态在C++中的实现
1. 执行
2. 静止就绪
3. 活动就绪
4. 静止阻塞
5. 活动阻塞

## 2.2 为什么有了进程还要线程，它们又有什么区别？
### 2.2.1 为什么有了进程还需要线程？
进程属于在CPU和系统资源等方面提供的抽象，能够有效提高CPU的利用率。
进程的缺陷主要体现在：<br>
1. 进程在一个时间做一件事情；
2. 进程如果在执行过程中发生阻塞，例如等待输入，整个进程就会挂起，即使进程中有些工作不依赖于输入的数据，也将无法执行。
3. 进程作为资源的拥有者，因而在创建、撤销和切换中，系统必须为之付出较大的时空开销。这就限制了系统中所能设置的进程数目，而且进程切换也不宜过于频繁，从而限制了并发程度的进一步提高。
	1. **创建** 系统在创建一个进程时，必须为它分配所需的、除处理机以外的所有资源，如内存空间、I/O设备，以及创建相应的PCB。
	2. **撤销** 系统在撤销一个进程时，必须对其所占有的资源执行回收操作，然后再撤销PCB。
	3. **切换** 对进程进行上下文切换时，需要保留当前进程的CPU环境，设置新选中进程的CPU环境，需要花费不少处理机时间。
线程是在进程这个层次上提供的一层并发的抽象：<br>
1. 线程能够使系统在同一时间做多件事情；
2. 当进程遇到阻塞挂起时，例如等待输入，线程能够使不依赖输入数据的工作继续执行；
3. 线程可以有效利用多处理器和多核计算机，在没有线程之前，多核并不能让一个进程的执行速度提高。
4. 进程是指一段正在执行的程序，线程有时也被称为轻量级进程，线程是程序执行的最小单元，一个进程可以拥有多个线程，各个线程之间共享程序的内存空间(代码段、数据段、堆空间)以及一些进程级的资源(比如打开的文件等)，但是每个线程拥有自己的栈空间。

### 2.2.2 进程和线程有什么区别？ 
1. **调度的基本单位** 线程是独立调度(能独立运行)的基本单位，线程切换时仅需保存和设置少量寄存器的内容，切换代价远低于进程。在同一进程中，线程的切换不会引起进程切换，从一个进程的线程切换到另一个进程中的线程时，会引起进程切换。
2. **并发性** 在引入线程的操作系统中，不仅进程之间可以并发执行，而且在一个进程之间的多个线程之间也可以并发执行。
3. **拥有资源** 进程可以拥有资源，并作为系统中拥有资源的一个基本单位。然而线程本身并不拥有系统资源，而是仅有一点必不可少的、能保证独立运行的资源。
4. **独立性** 在同一进程中的不同线程之间的独立性要比不同进程之间的独立性低的多。 线程有自己的栈空间和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮。
5. **系统开销** 系统在创建、撤销和切换进程时付出的开销比对线程执行相同操作的开销要大得多。
6. **通信** 线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助IPC(Inter-Process Communication/进程间通信)。
7. **支持多处理机系统** 在多处理机系统中，对于单线程进程，不管有多少处理机，该进程只能运行在一个处理机上。但对于多线程进程，就可以将一个进程中的多个线程分配到多个处理机上，使它们并行执行。

## 2.3 进程与线程各自是怎么同步的？
### 2.3.1 什么是进程同步，为什么需要进程同步？
进程同步是指为了完成某种任务而建立的多个进程，这些进程在合作的过程中需要协调工作次序进行**有序的访问**而出现等待所产生的制约关系。把异步环境下的一组并发进程因直接制约而互相发送消息而进行互相合作、互相等待，使得各进程按一定的速度执行的过程称为进程间的同步。<br>
在操作系统引入进程后，一方面可以是系统中的多道程序并发执行，这不仅能有效地改善资源利用率，还可以显著地提高系统吞吐量，但一方面却使系统变的更加复杂。如果不能采取有效措施，对多个进程的运行进行妥善的管理，必然会因为这些进程对系统资源的无序争夺给系统造成混乱。致使每次处理的结果存在着不确定性，即显现出不可再现性。

### 2.3.2 进程同步机制应该遵循的规则
1. **空闲让进** 当无进程处于临界区时，表明临界资源处于空闲状态，应允许一个请求进入临界区的进程立即进入自己的临界区，以有效地利用理解资源。 
2. **忙则等待** 当已有进程进入临界区时，表明临界资源正在被访问，因而其他试图进入临界区的资源必须等待，以保证对临界资源的互斥访问。
3. **有限等待** 对要求访问临界资源的进程，应保证在有限时间内能进入自己的临界区，以免陷入“死等”状态。
4. **让权等待** 当进程不能进入自己的临界区时，应立即释放处理机，以免进程陷入“忙等”状态。

### 2.3.3 进程的同步方式
#### 2.3.3.1 硬件同步
1. **关中断** 在进入锁测试之前关闭中断，直到完成锁测试并上锁之后才能代开中断。这样，进程在临界区执行期间，计算机系统不响应中断，从而不发生调度，也就不会发生进程或线程切换。由此保证了对锁的测试和关锁操作的连续性和完整性，有效地保证了互斥。
	1. 滥用关中断权力可能导致严重后果。
	2. 关中断时间过长会影响系统效率，限制了处理器交叉执行程序的能力。
	3. 关中断不适用与多CPU系统。

#### 2.3.3.2 信号量
1. 整型信号量  整型计数器表示资源数目。
2. 记录型信号量 整型计数器表示资源数目，进程链表指针链接所有等待进程。
3. AND型信号量 一个进程需要多个共享资源方能执行任务。 将进程在整个运行过程中需要的所有资源一次性全部分配给进程，带进程使用完后再一起释放。
4. 信号量集 对进程所申请的所有资源以及每类资源不同的资源需求量，在一次P、V原语操作中完成申请或者释放。
5. 利用信号量可以实现前驱关系。

6. 利用C++11标准库实现信号量
```cpp
// 整型信号量
// 一个整型值表示资源数目

#include <mutex>
#include <condition_variable>

class CppSemaphore {

private:
    std::condition_variable cv;
    std::mutex mutex;
    int value;
public:
    CppSemaphore(int init) :
            value(init) {
    }

    void wait() {
        std::unique_lock<std::mutex> lock(mutex);
        while (value < 1) {
            cv.wait(lock);
        }
        value--;
    }

    bool try_wait() {
        std::unique_lock<std::mutex> lock(mutex);
        if (value < 1)
            return false;
        value--;
        return true;
    }

    void post() {
        {
            std::unique_lock<std::mutex> lock(mutex);
            value++;
        }
        cv.notify_one();
    }
};
```

#### 2.3.3.3 管程
1. 信号量同步机制中每个要访问临界资源的进程都必须自备同步操作wait(S)和signal(S)，这样使得大量的同步操作分散在各个进程中，不仅给系统的管理带来麻烦，而且还会因为同步操作的使用不当导致系统死锁。
2. 管程利用共享数据结构抽象地表示系统中的共享资源，并且将对该共享数据结构实施的特定操作定义为一组过程。进程对共享资源的申请、释放和其他操作必须通过这组过程，间接地对共享数据结构实现操作。对于请求访问共享资源的诸多并发进程，可以根据资源的情况接收或者阻塞，确保每次仅有一个进程进入管程，执行这组过程，使用共享资源，达到对共享资源访问的统一管理，有效地实现了进程互斥。总而言之，管程就是**代表共享资源的数据结构以及由对该数据结构实施操作的一组过程所组成的资源管理程序共同构成了一个操作系统的资源管理模块。**
3. **管程中的条件变量** 如果一个进程调用了管程，在管程中时被阻塞或者挂起，直到阻塞或者挂起的原因解除，而在此期间，如果该进程不释放管程，则其他进程无法进入管程，被迫长时间的等待。条件变量X保存了一个链表，用于记录因该条件变量而阻塞的所有进程，同时提供的两个操作可表示为X.wait和X.signal;
	1. X.wait: 正在调用管程的进程因X条件需要被阻塞或者挂起，则调用X.wait将自己插入到X的等待队列上，并释放管程，直到X条件变化，此时其它进程可以使用该管程。
	2. X.signal: 正在调用管程的进程发现X条件发生了变化，则调用X.signal，重新启动一个因为X条件而被阻塞或者挂起的进程，如果存在多个这样的进程，则选择其中的一个，如果没有，继续执行原进程，而不产生任何后果。

4. 管程的内容：
	1. 名称；
	2. 局部于管程的共享数据结构说明；
	3. 对共享数据结构进行操作的一组过程；
	4. 对局部于管程的共享数据设置初始值的语句。



### 2.3.4 线程的同步方式
1. 临界区
2. 互斥量
3. 信号量
4. 事件对象


## 2.4 进程通信和线程通信
### 2.4.1  进程之间的通信方式
1. **系统IPC**
	1. **消息队列** 消息队列是消息的链表，存放在内核中。一个消息队列由一个标识符(队列ID)来标记。消息队列克服了信号传递信息少，管道只能承载无格式字节流以及缓冲区大小受限等特点。具有写权限的进程可以按照一定的规则向消息队列中添加新信息，对消息队列有读权限的进程可以从消息队列中读取信息。
		1. 消息队列是面向记录的，其中的消息具有特定的格式以及特定的优先级。
		2. 消息队列独立于发送和接收进程。进程终止时，消息队列及其内容并不会被删除。
		3. 消息队列可以实现消息的随机查询，消息不一定要以先进先出的次序读取，也可以按照消息类型读取。
	2. **信号量/Semaphores** 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。信号量用于实现进程间的互斥与同步，而不是用于存储进程间的通信数据。
		1. 信号量用于进程间同步，若要在进程间传递数据需要结合共享内存。
		2. 信号量基于系统的P(wait(S)) V(signal(S))操作，程序对信号量的操作都是原子操作。
		3. 每次对信号量的PV操作不仅限于对信号量值加1或者减1，而且可以加减任意正整数。
		4. 信号量集。
	3. **信号/signal**
	4. **共享内存/Shared Memory** 共享内存使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁或者信号量等。
		1. 共享内存是最快的一种IPC，因为进程是直接对内存进行存取。
		2. 因为是多个进程可以同时操作，所以需要同步。
		3. 信号量+共享内存通常结合在一起使用，信号量用来同步对共享内存的访问。

2. **管道**(PIPE)通信系统
管道是指用于连接一个读进程和一个写进程以实现它们之间通信的一个共享文件。向管道(共享文件)提供输入的发送进程(写进程)以字符流形式将大量数据送入管道；而接受管道输出的接收进程(读进程)则从管道中接收数据。管道主要包括无名管道和命名管道：管道可用于具有亲缘关系的父子进程间的通信，命名管道除了具有管道所具有的功能外，还允许无亲缘关系进程间的通信。
	1. 普通管道PIPE
		1. PIPE是半双工的，即数据只能在一个方向上流动，具有固定的读端和写端。
		2. PIPE只能用于具有亲缘关系的进程之间的通信(父子进程或者兄弟进程之间)。
		3. PIPE可以看成是一种特殊的文件，对于它的读写也可以使用普通的read、write等函数。但是它并不是普通文件，并不属于其他任何文件系统，并且只存在于内存中。
	2. 命名管道FIFO
		1. FIFO可以用于无关的进程之间交换数据。
		2. FIFO有路径名与之相关联，它以一种特殊设备文件形式存在于文件系统中。
管道机制必须具备的协调能力：
	1. 互斥，当一个进程正在对PIPE进行读/写操作时，其他进程必须等待。
	2. 同步，当写进程把一定数量的数据写尽PIPE，便去睡眠等待，知道读进程取走数据后再把写进程唤醒。
	3. 确定对方是否存在，只有确定对方已经存在了才能通信。

3. **客户机-服务器系统**
	1. 套接字(Socket) 



### 2.4.2 线程之间的通信方式
1. **临界区** 通过多线程的串行化来访问公共资源或者一段代码，速度快，适合控制数据访问。
2. **互斥量** 采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以保证公共资源不会被多个线程同时访问。
3. **信号量** 为控制具有有限数量的用户资源而设计，允许多个线程在同一时刻去访问一个资源，但一般需要限制同一时刻访问此资源的最大线程数目。
4. **事件/信号** 通过通知操作的方式来保持多线程同步，还可以方便地实现多线程优先级的比较操作。

### 2.4.3 进程通信和线程通信之间有什么区别？
1. 

### 2.4.4 Linux中的进程通信方式
1. Unix IPC包括管道(PIPE)、命名管道(FIFO)和信号(Signal)
	1. 管道是由内核管理的一个缓冲区。管道的一端连接一个进程的输出，这个进程会向管道中放入信息。管道的另一端连接一个进程的输入，这个进程取出被放入管道的信息。一个缓冲区不需要很大，它被设计成为环形的数据结构，以便管道可以被循环利用。当管道中没有信息的话，从管道中读取的进程会等待，直到另一端的进程放入信息。当管道被放满信息的时候，尝试放入信息的进程会等待，直到另一端的进程取出信息。当两个进程都终结的时候，管道也自动消失。
	2. 管道利用fork机制建立，从而让两个进程可以连接到同一个PIPE上。最开始的时候，PIPE的读写两端都连接在同一个进程。当fork复制进程的时候，会将这两个连接也复制到新的进程。随后，每个进程关闭自己不需要的一个连接。
	3. 
	4. FIFO (First in, First out)为一种特殊的文件类型，它在文件系统中有对应的路径。当一个进程以读(r)的方式打开该文件，而另一个进程以写(w)的方式打开该文件，那么内核就会在这两个进程之间建立管道，所以FIFO实际上也由内核管理，不与硬盘打交道。之所以叫FIFO，是因为命名管道本质上是一个先进先出的队列数据结构，最早放入的数据被最先读出来，从而保证信息交流的顺序。FIFO只是借用了文件系统(file system,命名管道是一种特殊类型的文件，因为Linux中所有事物都是文件，它在文件系统中以文件名的形式存在。)来为管道命名。写模式的进程向FIFO文件中写入，而读模式的进程从FIFO文件中读出。当删除FIFO文件时，管道连接也随之消失。FIFO的好处在于可以通过文件的路径来识别管道，从而让没有亲缘关系的进程之间建立连接。
	5. 

## 2.4 多进程与多线程有什么不同？
进程是资源分配的最小单位，而线程是CPU调度的最小单位。多线程之间共享同一个进程的地址空间，线程间通信简单，同步复杂，线程创建、销毁和切换简单，速度快，占用内存少，适用于多核分布式系统，但是线程间会相互影响，一个线程意外终止会导致同一个进程的其他线程也终止，程序可靠性弱。而多进程间拥有各自独立的地址运行空间，进程间不会相互影响，程序可靠性强，但是进程创建、销毁和切换复杂，速度慢，占用内存多，进程间通信复杂，但是同步简单，使用与多核多机分布式系统。
## 2.5 进程间如何通信？


## 2.6 僵尸进程和孤儿进程？
### 2.6.1 什么是正常进程、孤儿进程和僵尸进程？
1. **正常进程** 正常情况下，子进程是通过父进程创建的，子进程再去创建新的进程。子进程的结束和父进程的运行是一个异步的过程，即父进程永远无法预测子进程到底什么时候结束。当一个进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。UNIX提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息，就可以得到。在每个进程退出的时候，内核释放该进程的所有资源，包括打开的文件，占用的内存等。但是仍然为其保留一定的信息，直到父进程通过wait()/waitpid()来取时才释放。保存的信息包括：1. 进程号 2. 退出状态 3. 运行时间等。

2. **孤儿进程** 一个父进程退出，而它的一个或者多个子进程还在运行，那么这些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。

3. **僵尸进程** 一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait()或者waitpid()获取子进程的状态信息，那么子进程的状态描述符仍然会保存在系统中。这种进程称之为僵尸进程。
	1. 僵尸进程是一个进程必然会经过的过程，是每个子进程在结束时都要经过的阶段。
	2. 如果子进程在exit()之后，父进程没有来的及处理，这时用ps命令就能看到子进程的状态是“Z”(僵死状态)。如果父进程能及时处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。
	3. 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。

### 2.6.2 僵尸进程有什么危害？怎样解决这个问题？
如果进程不调用wait()/waitpid()的话，那么其保留的诸多信息就不会被释放，其进程号就会被一直占用着，但是系统所能使用的进程号是有限的，如果大量地产生僵尸进程，将因为没有可用的进程号而不能再产生新的进程。<br>
解决僵尸进程给系统带来的问题:
1. 外部消灭：通过kill发送SIGTERM或者SIGKILL信号消灭产生僵尸进程的进程，它产生的僵尸进程就变成了孤儿进程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源。
2. 内部解决：
	1. 子进程退出时向父进程发送SIGCHILD信号，父进程处理SIGCHILD信号。在信号处理函数中调用wait()进行处理僵尸进程。
	2. fork两次，原理是将子进程成为孤儿进程，从而其的父进程变为init进程，通过init进程可以处理僵尸进程。


## 2.7 死锁发生的条件是什么？如何解决死锁？
### 2.7.1 可抢占性资源与不可抢占性资源
1. **可抢占性资源** 可抢占性资源是指进程在获得这类资源后，该资源可以再被其他进程或者系统抢占，对于这类资源是不会引起死锁的。
2. **不可抢占性资源** 一旦系统把不可抢占资源分配给进程后，就不能将它强行收回，只能在进程用完后自行释放。

### 2.7.2 死锁发生的条件
如果一组进程中的每一个进程**都在**等待仅由该组进程中的其他进程才能引发的时间，那么该组进程就是死锁的。<br>
死锁是指两个或者两个以上的进程在执行过程中，因争夺资源而造成的相互等待的现象。死锁发生的四个必要条件如下:<br>
1. **互斥条件**： 进程对所分配到的资源不允许其他进程访问，若其他进程访问该资源，只能等待，直至占有该资源的进程使用完成后释放该资源；
2. **请求和保持条件**： 进程获得一定资源后，又对其他资源发出请求，但是该资源可能被其他进程占有。此时请求阻塞，但是该进程不会释放掉自己已经占有的资源；
3. **不可抢占条件**： 进程已经获得的资源，在未使用完成前，不可被抢占，只能在使用后自己释放；
4. **环路等待条件**： 进程发生死锁后，必然存在一个进程-资源之间的环形链。

### 2.7.3 死锁产生的情景举例


1. 多个线程抢占多个资源，每个线程申请了部分资源但是没有获得运行所需要的全部资源，而彼此等待。
```cpp
// 保证各线程中多个mutex的加锁顺序一致
class DeadLock{
public:
	DeadLock(){}
	~DeadLock(){}
	void thread1();
	void thread2();
private:
	std::mutex mutex1, mutex2;
};

void DeadLock::thread1(){
	mutex1.lock();
	std::chrono::milloseconds duration(2000);
	std::this_thread::sleep_for(duration); //此线程休眠，争取时间片轮换
	mutex2.lock();
	std::cout<<"thread 1 is running!"<<std::endl;
	mutex1.unlock();
	mutex2.unlock();
}

void DeadLock::thread2(){
	mutex2.lock();
	std::chrono::milloseconds duration(2000);
	std::this_thread::sleep_for(duration); //此线程休眠，争取时间片轮换
	mutex1.lock();
	std::cout<<"thread 2 is running!"<<std::endl;
	mutex2.unlock();
	mutex1.unlock();
}


int main(){
	DeadLock deadlock;
	std::thread t1(&DeadLock::thread1, &deadlock);
	std::thread t2(&DeadLock::thread2, &deadlock);
	t1.join();
	t2.join();
	return 0;
}
```

### 2.7.4 该如何解决死锁？
#### 2.7.4.1 死锁预防
破坏导致死锁发生的4个必要条件中的一个就可以预防死锁。
1. **破坏互斥条件** 无法破坏
2. **破坏请求和保持条件** 
	1. 协议一： 所有进程在开始运行之前，必须一次性地申请其所在整个运行过程中所需要的全部资源。此时如果系统有足够的资源分配给某进程，便可把其需要的所有资源分配给它，这样的话，该进程在运行期间便不会再提出资源要求，从而破坏了“请求”条件。系统在分配资源时，只要有一种资源不能满足进程的要求，即使其它所需的各资源都空闲也不分配给该线程，而让该线程等待。由于该线程在等待期间未占有任何资源，于是破坏了“保持”条件，从而可以防止死锁的发生。
	2. 协议二： 允许一个进程只获得运行初期所需的资源后就开始运行。运行过程中再逐步释放已分配给自己的、且已经使用完毕的全部资源，然后在请求新的所需的资源。


3. **破坏不可抢占条件** 当一个已经保持了某些不可被抢占资源的进程，提出了新的资源请求而不能得到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。
4. **破坏环路等待条件** 对系统的所有资源进行线性排序，并赋予不同的序号。规定每个进程必须按序号递增的顺序请求资源，假如某进程已经请求到一些序号比较高的资源，后来又想请求一个序号低的资源时，必须先释放所有具有相同或者更高序号的资源后，才能申请序号低的资源。

#### 2.7.4.2 死锁避免
1. **安全状态和不安全状态** 

#### 2.7.4.3 死锁检测

#### 2.7.4.4 死锁解除

## 2.8 并发和并行
1. **并发**是指宏观上看起来两个程序在同时运行，比如说在单核CPU上的多任务。但是从微观上看两个程序的指令是交织着运行的，你的指令之间穿插着我的指令，我的指令之间穿插着你的，在单个周期内只运行了一个指令。这种并发并不能提高计算机的性能，只能提高效率。
2. **并行**是指严格物理意义上的同时运行，比如多核CPU，两个程序分别运行在两个核上，两者之间互不影响，单个周期内每个程序都运行了自己的指令，也就是运行了两条指令。这样来说并行的确提高了计算机的效率，所以现在的CPU都是往多核方面发展。 

## 2.9 进程调度
### 2.9.1 进程调度的主要任务
1. **保存处理机的现场信息** 在进行调度时首先需要保存当前进程的处理机的现场信息，如程序计数器、多个通用寄存器中的内容等等。
2. **按照某种算法选择进程** 调度程序按照某种算法从就绪队列中选取一个进程，将其状态改为就绪状态，把准备把处理机分配给它。
3. **把处理器分配给进程** 由分派程序把处理器分配该进程，需要将选中进程的进程控制块内有关处理现场的信息装入处理器相应的各个寄存器中，把处理器的控制权交予该进程，让它从上次的断点处恢复运行。

### 2.9.2 


# 3 存储器管理
## 3.1 存储器的层次结构
1. CPU寄存器
2. 主存(高速缓存、主存储器、磁盘缓存)
3. 辅存(固定磁盘、可移动存储介质)<br>
寄存器、高速缓存、主存储器和磁盘缓存属于操作系统存储管理的管辖范畴，掉电后他们中存储的信息不再存在。而低层的固定磁盘和可移动存储介质属于设备管理的管辖范畴，它们存储的信息将被长期保存。

## 3.2 分页存储管理方式
将用户程序的地址空间分为若干个固定大小的区域，称为“页”。相应地，也将内存空间分为若干个物理框或者页框，页和块的大小相同，这样可以将用户程序的任一页放入任一物理块中，实现了离散分配。
### 3.2.1 分页存储管理的基本原理
1. **页面和物理块** 分页存储管理将进程的逻辑地址空间分成若干个页，并为各页加以编号。相应地，也把内存的物理地址空间分成若干个块，同样加以编号。在为进程分配内存时，以块为单位，将进程中的若干页分别装入到多个可以不相邻的物理块中。
2. **地址结构** 分页地址中的地址结构由页号和位移量(页内地址)构成。
3. **页表** 在分页系统中，允许将进程的各个页离散地存储在内存的任一物理块中，为保证进程仍然能够正确地运行，即能在内存中找到每个页面对应的物理块，系统又为每个进程建立了一张页表。在进程地址空间中的所有页，依次在也页表中有一张页表项，其中记录了相应页在内存中对应的物理块号。

### 3.2.2 分页存储管理的地址变换机构
1. 基本的地址变换结构
2. 具有块表的地址变换结构

## 3.3 分段存储管理方式
将用户程序的地址空间分为若干个大小不同的段，每段可以定义一组相对完整的信息。在存储时，以段为单位，这些段在内存中可以不相邻接，这样也实现了离散分配。

### 3.3.1 分段存储管理的基本原理
1. **分段** 在分段存储管理方式中，作业的地址空间被划分为若干段，每个段定义了一组逻辑信息。每个段都从0开始编址，并采用一段连续的地址空间。段的长度由相应的逻辑信息组的长度决定，因此各段的长度并不相等。整个作业的地址空间被分成多个段，所以呈现出二维特性，亦即，每个段即包含了一部分地址空间，又标识了逻辑关系。其逻辑地址由段号和段内地址所组成。
2. **段表** 段映射表，用于实现从逻辑段到物理内存的映射。

### 3.3.2 分段存储管理的地址变换机构

### 3.3.3 信息共享

### 3.3.4 分段存储管理方式的优点
1. **方便编程** 
2. **信息共享** 在实现对程序和数据的共享时，是以信息的逻辑单位为基础的。
3. **信息保护** 信息保护同样是以信息的逻辑单位为基础的，而且经常是以一个过程、函数或文件为基本单位进行保护的。
4. **动态增长** 分段存储管理方式可以应对实际应用中一些段大小动态增长的问题。
5. **动态链接** 动态链接要求的是以目标程序(段)作为链接的基本单位。

## 3.4 段页式存储管理方式
### 3.4.1 分段和分页有什么区别？
1. **页是信息的物理单位，段是信息的逻辑单位。**采用分页存储管理方式是为了实现离散分配方式，以消减内存的外零头，提高内存的利用率。或者说，分页仅仅是系统管理上的需要，完全是系统行为，对用户是不可见的。分段式存储管理方式中的段是信息的逻辑单位，它通常包含的是一组意义相对完整的信息。分段的目的主要在于能够更好地满足用户需要。
2. **页的大小固定且由系统决定，段的长度不固定，决定于用户所编写的程序。**在采用分页存储管理方式的系统中，在硬件结构上，就把用户程序的逻辑地址划分为页号和页内地址两部分，也就是说直接由硬件实现的，因而在每个系统中只能有一种大小的页面。而段的大小不固定，决定于用户所编写的程序，通常由编译程序在对源程序进行编译时，根据信息的性质来划分。
3. **分页的用户程序地址空间是一维的，分段系统的用户程序地址空间是二维的。** 分页完全是系统行为，故在分页系统中用户程序的地址空间是单一的线性地址空间，程序员只需要利用一个记忆符即可以表示一个地址。而分段是用户行为，故在分段系统中，用户程序的地址空间是二维的，程序员在标识一个地址时，既需要给出段名，又需要给出段内地址。

### 3.4.2 段页式存储管理的基本原理

### 3.4.3 段页式存储管理的地址变换过程

# 4 虚拟存储器
## 4.1 虚拟存储器若干基本问题
### 4.1.1 虚拟存储器定义
虚拟存储器是指一种具有请求调入功能和置换功能，能从逻辑上对内存容量加以扩充的一种存储器系统。其逻辑容量由内存容量和外存容量之和所决定，其运行速度接近于内存速度，而每位的成本却又接近于外存。
### 4.1.2 虚拟存储器特征 
1. **多次性** 多次性是相对于传统存储器管理方式的一次性而言的，是指一个作业中的程序和数据无需在作业运行时一次性地全部装入内存，而是允许被分成多次调入内存运行，即只需将当前要运行的那部分程序和数据装入内存即可开始运行。以后每当要运行到尚未调入的那部分程序时，再将其调入。
2. **对换性** 对换性是相对于传统存储器管理方式的常驻性而言的，是指一个作业中的程序和数据，无须在作业运行时一直常驻内存，而是允许在作业的运行过程中进行换进、换出，亦即在程序运行期间，允许将那些暂不使用的代码和数据从内存调至外存的对换区(换出)，待以后需要时再将它们从外存调至内存(换进)。甚至还允许将暂时不运行的进程调至外存，待它们重又具备运行条件时再调入内存。
3. **虚拟性** 虚拟性是指能够从逻辑上扩充内存容量，使用户所看到的内存容量远大于实际内存容量。虚拟性以多次性和对换性为基础。

## 4.2 请求分页存储管理方式
请求分页系统是建立在基本分页基础上的，为了能支持虚拟存储器功能，而增加了请求调页功能和页面置换功能，相应地，每次调入和换出的基本单位都是长度固定的页面。
### 4.2.1 硬件支持
1. 请求页表机制
2. 缺页中断机构
3. 地址变换机构

### 4.2.2 内存分配
1. 最小物理块数的确定
2. 内存分配策略
3. 物理块分配算法

### 4.2.3 页面调入策略
1. 何时调入页面
2. 从何处调入页面
3. 页面调入过程
4. 缺页率

## 4.3 页面置换算法
### 4.3.1 最佳置换算法

### 4.3.2 先进先出置换算法

### 4.3.3 LRU(Least Recently Used/最近最久未使用置换算法)
#### 4.3.3.1 LRU的原理
#### 4.3.3.2 LRU的优缺点
#### 4.3.3.3 手写LRU
```cpp
#include <list>
#include <unordered_map>
#include <cassert>   
using namespace std;  
struct Element
{
    int key;
    int value;
    Element(int k, int v):key(k), value(v){}
};
class LRUCache {
private:
    list<Element> m_list;
    unordered_map<int, list<Element>::iterator> m_map;
    int m_capacity;
public:
    LRUCache(int capacity) {
        m_capacity = capacity;
    }

    int get(int key) {
        if (m_map.find(key) == m_map.end())
            return -1;
        else
        {
            //将元素放入链表头部
            m_list.splice(m_list.begin(), m_list, m_map[key]);
            m_map[key] = m_list.begin();
            return m_list.begin()->value;
        }
    }

    void put(int key, int value) {
        assert(m_capacity > 0);
        if (m_map.find(key) != m_map.end())
        {   //更value
            m_map[key]->value = value;
            //将元素放入链表头部
            m_list.splice(m_list.begin(), m_list, m_map[key]);
            m_map[key] = m_list.begin();
        }
        else if (m_capacity == m_list.size())
        {
            m_map.erase(m_list.back().key);
            m_list.pop_back();
            m_list.push_front(Element(key, value));
            m_map[key] = m_list.begin();
        }
        else
        {
            m_list.push_front(Element(key, value));
            m_map[key] = m_list.begin();
        }
    }
};

/**
 * Your LRUCache object will be instantiated and called as such:
 * LRUCache obj = new LRUCache(capacity);
 * int param_1 = obj.get(key);
 * obj.put(key,value);
 */
```

### 4.3.4 LFU(Least Frequently Used/最少使用置换算法)

### 4.3.5 CLOCK置换算法

### 4.3.6 页面缓冲算法

## 4.4 请求分段存储管理方式
### 4.4.1 硬件支持
1. 请求段表机制
2. 缺段中断机构
3. 地址变换机构

### 4.4.2 分段的共享与保护
1. 共享段表
2. 共享段的分配与回收
3. 分段保护


# 5. 数据存储

## 5.1 大端存储与小端存储

### 5.1.1 什么是“大端模式”和“小端模式”？
1. 大端模式（Big-endian），是指数据的高字节，保存在内存的低地址中，而数据的低字节，保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放；
2. 小端模式（Little-endian）, 是指数据的高字节保存在内存的高地址中,而数据的低字节保存在内在的低地址中,这种存储模式将地址的高低和数据位 权有效结合起来,高地址部分权值高,低地址部分权值低,和我们的逻辑方法一致;

### 5.1.2 为什么有“大端模式”和“小端模式”之分？
因为在计算机系统中，我们是以字节为单位的，每个地址单元都对应着一个字节，一个字节为 8bit。但是在C语言中除了8bit的char之外，还有16bit的short型，32bit的long型（要看具体的编译器），另外，对于位数大于 8位的处理器，例如16位或者32位的处理器，由于寄存器宽度大于一个字节，那么必然存在着一个如何将多个字节安排的问题。因此就导致了大端存储模式和小端存储模式。我们常用的X86结构是小端模式，而KEIL C51则为大端模式。很多的ARM，DSP都为小端模式。有些ARM处理器还可以由硬件来选择是大端模式还是小端模式。
